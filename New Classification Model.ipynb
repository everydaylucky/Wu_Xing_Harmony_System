{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [100/2000], Train Loss: 1.1798, Train Accuracy: 0.8194, Val Loss: 1.1205, Val Accuracy: 0.8556, Best Val Accuracy: 0.8556\n",
      "Epoch [200/2000], Train Loss: 0.9613, Train Accuracy: 0.9833, Val Loss: 1.0191, Val Accuracy: 0.8778, Best Val Accuracy: 0.8889\n",
      "Epoch [300/2000], Train Loss: 0.9423, Train Accuracy: 0.9861, Val Loss: 1.0214, Val Accuracy: 0.8778, Best Val Accuracy: 0.9000\n",
      "Epoch [400/2000], Train Loss: 0.9368, Train Accuracy: 0.9917, Val Loss: 1.0162, Val Accuracy: 0.8778, Best Val Accuracy: 0.9000\n",
      "Epoch [500/2000], Train Loss: 0.9246, Train Accuracy: 0.9972, Val Loss: 1.0260, Val Accuracy: 0.8778, Best Val Accuracy: 0.9111\n",
      "Epoch [600/2000], Train Loss: 0.9255, Train Accuracy: 1.0000, Val Loss: 1.0316, Val Accuracy: 0.8667, Best Val Accuracy: 0.9111\n",
      "Epoch [700/2000], Train Loss: 0.9202, Train Accuracy: 1.0000, Val Loss: 1.0347, Val Accuracy: 0.8778, Best Val Accuracy: 0.9111\n",
      "Epoch [800/2000], Train Loss: 0.9173, Train Accuracy: 1.0000, Val Loss: 1.0326, Val Accuracy: 0.8667, Best Val Accuracy: 0.9111\n",
      "Epoch [900/2000], Train Loss: 0.9189, Train Accuracy: 1.0000, Val Loss: 1.0335, Val Accuracy: 0.8778, Best Val Accuracy: 0.9222\n",
      "Epoch [1000/2000], Train Loss: 0.9183, Train Accuracy: 1.0000, Val Loss: 1.0329, Val Accuracy: 0.8556, Best Val Accuracy: 0.9222\n",
      "Epoch [1100/2000], Train Loss: 0.9176, Train Accuracy: 1.0000, Val Loss: 1.0298, Val Accuracy: 0.8667, Best Val Accuracy: 0.9222\n",
      "Epoch [1200/2000], Train Loss: 0.9215, Train Accuracy: 1.0000, Val Loss: 1.0385, Val Accuracy: 0.8667, Best Val Accuracy: 0.9222\n",
      "Epoch [1300/2000], Train Loss: 0.9195, Train Accuracy: 1.0000, Val Loss: 1.0276, Val Accuracy: 0.8889, Best Val Accuracy: 0.9222\n",
      "Epoch [1400/2000], Train Loss: 0.9242, Train Accuracy: 1.0000, Val Loss: 1.0378, Val Accuracy: 0.8667, Best Val Accuracy: 0.9222\n",
      "Epoch [1500/2000], Train Loss: 0.9192, Train Accuracy: 1.0000, Val Loss: 1.0204, Val Accuracy: 0.8889, Best Val Accuracy: 0.9222\n",
      "Epoch [1600/2000], Train Loss: 0.9286, Train Accuracy: 1.0000, Val Loss: 1.0247, Val Accuracy: 0.9000, Best Val Accuracy: 0.9222\n",
      "Epoch [1700/2000], Train Loss: 0.9195, Train Accuracy: 1.0000, Val Loss: 1.0259, Val Accuracy: 0.8889, Best Val Accuracy: 0.9222\n",
      "Epoch [1800/2000], Train Loss: 0.9219, Train Accuracy: 1.0000, Val Loss: 1.0413, Val Accuracy: 0.8667, Best Val Accuracy: 0.9222\n",
      "Epoch [1900/2000], Train Loss: 0.9273, Train Accuracy: 1.0000, Val Loss: 1.0103, Val Accuracy: 0.9333, Best Val Accuracy: 0.9333\n",
      "Epoch [2000/2000], Train Loss: 0.9159, Train Accuracy: 1.0000, Val Loss: 1.0208, Val Accuracy: 0.9000, Best Val Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load and preprocess data\n",
    "df2 = pd.read_excel(\"Features1D/Features.xlsx\")\n",
    "X2 = df2.drop(['mood'], axis=1)\n",
    "y2 = df2['mood']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y2 = le.fit_transform(y2)\n",
    "\n",
    "# Encode categorical features\n",
    "X2['scale'] = le.fit_transform(X2['scale'])\n",
    "X2['key'] = le.fit_transform(X2['key'])\n",
    "\n",
    "# Split data\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, train_size=0.9, random_state=13, shuffle=True, stratify=y2)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_train, y2_train, train_size=0.8, random_state=13, shuffle=True, stratify=y2_train)\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X2_train_scaled = scaler.fit_transform(X2_train.drop('file', axis=1))\n",
    "X2_val_scaled = scaler.transform(X2_val.drop('file', axis=1))\n",
    "X2_test_scaled = scaler.transform(X2_test.drop('file', axis=1))\n",
    "\n",
    "# Convert to PyTorch tensors with correct shape\n",
    "X2_train_tensor = torch.tensor(X2_train_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y2_train_tensor = torch.tensor(y2_train, dtype=torch.long)\n",
    "X2_val_tensor = torch.tensor(X2_val_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y2_val_tensor = torch.tensor(y2_val, dtype=torch.long)\n",
    "X2_test_tensor = torch.tensor(X2_test_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y2_test_tensor = torch.tensor(y2_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X2_train_tensor, y2_train_tensor)\n",
    "val_dataset = TensorDataset(X2_val_tensor, y2_val_tensor)\n",
    "test_dataset = TensorDataset(X2_test_tensor, y2_test_tensor)\n",
    "\n",
    "batchSize = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchSize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define the model\n",
    "class Conv1DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=7, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=7, padding='same')\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Adjust the number of units in the first fully connected layer based on the input shape\n",
    "        self.fc1 = nn.Linear(32 * (X2_train_scaled.shape[1] // 8), 128)\n",
    "        self.dropout1 = nn.Dropout(0.7)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.bn1(torch.relu(self.conv1(x))))\n",
    "        x = self.pool2(self.bn2(torch.relu(self.conv2(x))))\n",
    "        x = self.pool3(self.bn3(torch.relu(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move to GPU\n",
    "model = Conv1DModel().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-2)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_accuracy += (predicted == y_batch).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_loss += criterion(val_outputs, y_val_batch).item()\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            val_accuracy += (predicted == y_val_batch).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy /= len(val_loader.dataset)\n",
    "\n",
    "    # Update best validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "              f'Best Val Accuracy: {best_val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [100/2000], Train Loss: 1.1328, Train Accuracy: 0.8472, Val Loss: 1.1486, Val Accuracy: 0.8556, Best Val Accuracy: 0.8556\n",
      "Epoch [200/2000], Train Loss: 0.9453, Train Accuracy: 0.9861, Val Loss: 1.0122, Val Accuracy: 0.9000, Best Val Accuracy: 0.9111\n",
      "Epoch [300/2000], Train Loss: 0.9303, Train Accuracy: 0.9889, Val Loss: 1.0168, Val Accuracy: 0.9000, Best Val Accuracy: 0.9111\n",
      "Epoch [400/2000], Train Loss: 0.9235, Train Accuracy: 0.9972, Val Loss: 1.0227, Val Accuracy: 0.8889, Best Val Accuracy: 0.9111\n",
      "Epoch [500/2000], Train Loss: 0.9243, Train Accuracy: 0.9972, Val Loss: 1.0249, Val Accuracy: 0.8889, Best Val Accuracy: 0.9111\n",
      "Epoch [600/2000], Train Loss: 0.9187, Train Accuracy: 1.0000, Val Loss: 1.0254, Val Accuracy: 0.9111, Best Val Accuracy: 0.9222\n",
      "Epoch [700/2000], Train Loss: 0.9255, Train Accuracy: 0.9972, Val Loss: 1.0328, Val Accuracy: 0.8889, Best Val Accuracy: 0.9222\n",
      "Epoch [800/2000], Train Loss: 0.9194, Train Accuracy: 1.0000, Val Loss: 1.0277, Val Accuracy: 0.9000, Best Val Accuracy: 0.9222\n",
      "Epoch [900/2000], Train Loss: 0.9203, Train Accuracy: 1.0000, Val Loss: 1.0185, Val Accuracy: 0.9222, Best Val Accuracy: 0.9333\n",
      "Epoch [1000/2000], Train Loss: 0.9186, Train Accuracy: 0.9972, Val Loss: 1.0298, Val Accuracy: 0.9000, Best Val Accuracy: 0.9333\n",
      "Epoch [1100/2000], Train Loss: 0.9202, Train Accuracy: 1.0000, Val Loss: 1.0170, Val Accuracy: 0.9000, Best Val Accuracy: 0.9333\n",
      "Epoch [1200/2000], Train Loss: 0.9165, Train Accuracy: 1.0000, Val Loss: 1.0117, Val Accuracy: 0.9000, Best Val Accuracy: 0.9444\n",
      "Epoch [1300/2000], Train Loss: 0.9183, Train Accuracy: 1.0000, Val Loss: 1.0203, Val Accuracy: 0.8778, Best Val Accuracy: 0.9444\n",
      "Epoch [1400/2000], Train Loss: 0.9190, Train Accuracy: 1.0000, Val Loss: 1.0158, Val Accuracy: 0.9000, Best Val Accuracy: 0.9556\n",
      "Epoch [1500/2000], Train Loss: 0.9190, Train Accuracy: 1.0000, Val Loss: 1.0253, Val Accuracy: 0.8889, Best Val Accuracy: 0.9556\n",
      "Epoch [1600/2000], Train Loss: 0.9171, Train Accuracy: 1.0000, Val Loss: 1.0148, Val Accuracy: 0.9333, Best Val Accuracy: 0.9556\n",
      "Epoch [1700/2000], Train Loss: 0.9210, Train Accuracy: 1.0000, Val Loss: 1.0013, Val Accuracy: 0.9444, Best Val Accuracy: 0.9556\n",
      "Epoch [1800/2000], Train Loss: 0.9188, Train Accuracy: 1.0000, Val Loss: 1.0033, Val Accuracy: 0.9222, Best Val Accuracy: 0.9556\n",
      "Epoch [1900/2000], Train Loss: 0.9237, Train Accuracy: 1.0000, Val Loss: 1.0135, Val Accuracy: 0.9222, Best Val Accuracy: 0.9556\n",
      "Epoch [2000/2000], Train Loss: 0.9183, Train Accuracy: 1.0000, Val Loss: 1.0166, Val Accuracy: 0.9111, Best Val Accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load and preprocess data\n",
    "df2 = pd.read_excel(\"Features1D/Features.xlsx\")\n",
    "X2 = df2.drop(['mood'], axis=1)\n",
    "y2 = df2['mood']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y2 = le.fit_transform(y2)\n",
    "\n",
    "# Encode categorical features\n",
    "X2['scale'] = le.fit_transform(X2['scale'])\n",
    "X2['key'] = le.fit_transform(X2['key'])\n",
    "\n",
    "# Split data\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, train_size=0.9, random_state=13, shuffle=True, stratify=y2)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_train, y2_train, train_size=0.8, random_state=13, shuffle=True, stratify=y2_train)\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X2_train_scaled = scaler.fit_transform(X2_train.drop('file', axis=1))\n",
    "X2_val_scaled = scaler.transform(X2_val.drop('file', axis=1))\n",
    "X2_test_scaled = scaler.transform(X2_test.drop('file', axis=1))\n",
    "\n",
    "# Convert to PyTorch tensors with correct shape\n",
    "X2_train_tensor = torch.tensor(X2_train_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y2_train_tensor = torch.tensor(y2_train, dtype=torch.long)\n",
    "X2_val_tensor = torch.tensor(X2_val_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y2_val_tensor = torch.tensor(y2_val, dtype=torch.long)\n",
    "X2_test_tensor = torch.tensor(X2_test_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y2_test_tensor = torch.tensor(y2_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X2_train_tensor, y2_train_tensor)\n",
    "val_dataset = TensorDataset(X2_val_tensor, y2_val_tensor)\n",
    "test_dataset = TensorDataset(X2_test_tensor, y2_test_tensor)\n",
    "\n",
    "batchSize = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchSize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define the model\n",
    "class Conv1DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=7, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=7, padding='same')\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Adjust the number of units in the first fully connected layer based on the input shape\n",
    "        self.fc1 = nn.Linear(32 * (X2_train_scaled.shape[1] // 8), 128)\n",
    "        self.dropout1 = nn.Dropout(0.6)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.bn1(torch.relu(self.conv1(x))))\n",
    "        x = self.pool2(self.bn2(torch.relu(self.conv2(x))))\n",
    "        x = self.pool3(self.bn3(torch.relu(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move to GPU\n",
    "model = Conv1DModel().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-2)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_accuracy += (predicted == y_batch).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy /= len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_loss += criterion(val_outputs, y_val_batch).item()\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            val_accuracy += (predicted == y_val_batch).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy /= len(val_loader.dataset)\n",
    "\n",
    "    # Update best validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "              f'Best Val Accuracy: {best_val_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
